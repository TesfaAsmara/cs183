{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Progress for week of 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 14:06:32 WARN Utils: Your hostname, Tesfas-MacBook-Pro-738.local resolves to a loopback address: 127.0.0.1; using 172.28.233.209 instead (on interface en0)\n",
      "23/09/18 14:06:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/18 14:06:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I demonstrate what I believe to be are at least 4 different operations (collect, filter, groupby, avg, and applyInPandas) on one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color='red', fruit='banana', v1=1, v2=10),\n",
       " Row(color='blue', fruit='banana', v1=2, v2=20),\n",
       " Row(color='red', fruit='carrot', v1=3, v2=30),\n",
       " Row(color='blue', fruit='grape', v1=4, v2=40),\n",
       " Row(color='red', fruit='carrot', v1=5, v2=50),\n",
       " Row(color='black', fruit='carrot', v1=6, v2=60),\n",
       " Row(color='red', fruit='banana', v1=7, v2=70),\n",
       " Row(color='red', fruit='grape', v1=8, v2=80)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sparkenv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6|  0|\n",
      "| blue|banana|  2|-10|\n",
      "| blue| grape|  4| 10|\n",
      "|  red|banana|  1|-38|\n",
      "|  red|carrot|  3|-18|\n",
      "|  red|carrot|  5|  2|\n",
      "|  red|banana|  7| 22|\n",
      "|  red| grape|  8| 32|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# def plus_mean(pandas_df):\n",
    "#     return pandas_df.assign(v2=pandas_df.v2 - pandas_df.v2.mean())\n",
    "\n",
    "# df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1814594154.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    opt/homebrew/Caskroom/miniforge/base/envs/sparkenv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229:(FutureWarning:, is_categorical_dtype, is, deprecated, and, will, be, removed, in, a, future, version., Use, isinstance(dtype,, CategoricalDtype), instead)\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# /opt/homebrew/Caskroom/miniforge/base/envs/sparkenv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
    "\n",
    "# +-----+------+---+---+\n",
    "# |color| fruit| v1| v2|\n",
    "# +-----+------+---+---+\n",
    "# |black|carrot|  6|  0|\n",
    "# | blue|banana|  2|-10|\n",
    "# | blue| grape|  4| 10|\n",
    "# |  red|banana|  1|-38|\n",
    "# |  red|carrot|  3|-18|\n",
    "# |  red|carrot|  5|  2|\n",
    "# |  red|banana|  7| 22|\n",
    "# |  red| grape|  8| 32|\n",
    "# +-----+------+---+---+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I begin to think about manipulating key Spark configurations such as the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialNumPartitions = df.rdd.getNumPartitions()\n",
    "df2 = df.repartition(numPartitions=3)\n",
    "manipulatedNumPartitions = df2.rdd.getNumPartitions()\n",
    "print(initialNumPartitions, manipulatedNumPartitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('sparkenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8af23d0cf4af99dfae7abd1f1b9394f577b0bf3f3e1585742e30428f1569d8b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
